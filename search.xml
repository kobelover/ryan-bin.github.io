<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[java和python中的Concurrent包的future异步]]></title>
    <url>%2F2017%2F09%2F17%2Fjava%E5%92%8Cpython%E4%B8%AD%E7%9A%84Concurrent%E5%8C%85%E7%9A%84future%E5%BC%82%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[本文主要阐述了java和python中concurrent包下的future异步，对于python中的concurrent.futures是在借鉴java的java.util.concurrent包的思想而成，然而python多线程中又有GIL的限制，所以下面就来看看两个语言下的concurrent下的future异同 接口的简单认识java中的future 是java.util.concurrent包下面的一个接口，该接口定义如下源码12345678public interface Future&lt;V&gt; &#123; boolean cancel(boolean mayInterruptIfRunning); boolean isCancelled(); boolean isDone(); V get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125; Python定义的是一个类源码123456789101112131415161718192021class Future(object): """Represents the result of an asynchronous computation.""" def __init__(self): self._condition = threading.Condition() self._state = PENDING self._result = None self._exception = None self._waiters = [] self._done_callbacks = [] def _invoke_callbacks(self): def __repr__(self): def cancelled(self): def running(self): def done(self): def __get_result(self): def add_done_callback(self, fn): def result(self, timeout=None): def exception(self, timeout=None): def set_running_or_notify_cancel(self): def set_result(self, result): def set_exception(self, exception): 语言特性带来的差异java在定义的时候定义的是一个接口，进行规范，在实现上比较自由，比如spring 框架的并发包：org.springframework.util.concurrent 就是继承Future这个抽象类。python定义的是一个类(虽然有abc但没用)，也就是在实现的时候，虽然可以去继承，但是为了解除冗余性，往往自己重新去实现了一下Future, 比如 asyncio.Future就是重新设计的，所以这一点来说java面向接口编程特点比Python还是要强很多，在面对各种并发问题的时候，java优势就更加明显 相同点两个语言 Future 可以从 Executor（java 是Executors）下的submit() 实例化得到， 但是java的特性，Executors提供更丰富的接口 java 的Executors接口：python的Executor接口： python 的concurrent.futures 却只有ProcessPoolExecutor和ThreadPoolExecutor， 目的也简单，前者主要解决 CPU密集问题，后者主要解决IO密集问题，因为在执行阻塞型 I/O 操作的函数， 在等待操作系统返回结果时都会释放 GIL， 所以Python中的多线程此刻才体现优势（相对于串行而言） 下面来简单的实验一下java的实现:123456789101112131415161718192021222324252627282930313233import java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.Future;public class demo &#123; public static void main(String[] args) throws ExecutionException, InterruptedException &#123; final int processors = Runtime.getRuntime().availableProcessors(); final ExecutorService exec = Executors.newFixedThreadPool(processors); Callable call = new Callable()&#123; public String call() throws Exception&#123; System.out.println("Hey, Man! this is thread1!!"); Thread.sleep(1000 * 3); return "This is thread1 and result is string, you can return other object"; &#125; &#125;; Future thead1 = exec.submit(call); //主线程设置了2秒，thead1 要sleep 3秒，但是两者是同时执行的 Thread.sleep(1000 * 2); System.out.println("this is main thread "); //其他不重要的事情 String res = (String) thead1.get(); System.out.println(res); //关闭线程池 exec.shutdown(); /* 打印：*/ //Hey, Man! this is thread1!! //this is main thread //This is thread1 and result is string, you can return other object &#125;&#125; python的实现：12345678910111213141516# coding=utf-8import concurrent.futuresimport timedef task(): print("Hey, Man! this is thread1!!") return "This is thread1 and result is string, you can return other object"def learn(): with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: time.sleep(3) thread1 = executor.submit(task) print(thread1.result())if __name__ == '__main__': learn() 同时可以通过下面的代码来比较一下python中的ProcessPoolExecutor和ThreadPoolExecutor ：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109# coding=utf-8import concurrent.futuresimport datetimeclass Number(object): """Number class""" def __init__(self): super(Number, self).__init__() self.futures = set() @property def one_func(self): for i in range(1, 10): yield i @property def two_func(self): for i in range(1, 10): yield i*2 @property def three_func(self): for i in range(1, 10): yield i*3 def n_func(self, n): def inner(): for i in range(1, 10): yield i*n return set(inner()) def share_value(self, n): """ 用于检测一个变量在并发中是否共享 """ def inner(): for i in range(1, 10): yield i return self.futures.update(set(inner())) # ThreadPoolExecutor def learn(self): start_time = datetime.datetime.now() with concurrent.futures. ThreadPoolExecutor(max_workers=8) as executor: tasks = [self.one_func, self.two_func, self.three_func] future_task = &#123;executor.submit(task): task for task in tasks&#125; for future in concurrent.futures.as_completed(future_task): number = future_task[future] for i in number: print(i) end_time = datetime.datetime.now() print(end_time - start_time) #range(1, 10): 0:00:00.001001 range(1, 100000): 0:00:01.386313 # ProceessProolExecutor def train(self): start_time = datetime.datetime.now() with concurrent.futures. ProcessPoolExecutor(max_workers=8) as executor: future_task = &#123;executor.submit(self.n_func, i): i for i in range(1, 8)&#125; for future in concurrent.futures.as_completed(future_task): number = future_task[future] print(number) print(future.result()) end_time = datetime.datetime.now() print(end_time-start_time) # range(1, 10):0:00:00.297790 range(1, 100000): 0:00:00.465234 # 在并发中因为开启了多个 interpreter, 所以数据是不共享的 def con_shared_value_test(self): ''' 此处用的是实例化开始的时候就加入的变量作为接受数据变量 ''' start_time = datetime.datetime.now() with concurrent.futures. ProcessPoolExecutor(max_workers=8) as executor: future_task = &#123;executor.submit(self.share_value, i): i for i in range(1, 8)&#125; for future in concurrent.futures.as_completed(future_task): number = future_task[future] print(number) # n print(future.result()) # None end_time = datetime.datetime.now() print("耗时：",(end_time-start_time)) # range(1, 10):耗时： 0:00:00.308821 range(1, 100000) 耗时： 0:00:00.060158 # 多线程把数据发到一个变量 def multiThread_share_value(self): start_time = datetime.datetime.now() with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: tasks = [self.one_func, self.two_func, self.three_func] future_task = &#123;executor.submit(task): task for task in tasks&#125; for future in concurrent.futures.as_completed(future_task): number = future_task[future] self.futures.update(set(number)) end_time = datetime.datetime.now() print("耗时：",(end_time-start_time)) # range(1, 10):耗时： 0:00:00.001977 range(1, 100000) 耗时： 0:00:00.049100 # 多进程把数据发到一个变量 def multiProcess_share_value(self): start_time = datetime.datetime.now() with concurrent.futures. ProcessPoolExecutor(max_workers=8) as executor: future_task = &#123;executor.submit(self.n_func, i): i for i in range(1, 8)&#125; for future in concurrent.futures.as_completed(future_task): number = future_task[future] self.futures.update(set(future.result())) end_time = datetime.datetime.now() print("耗时：",(end_time-start_time)) # range(1, 10):耗时：耗时： 0:00:00.297792 range(1, 100000) 耗时： 0:00:00.402069if __name__ == '__main__': n = Number() n.multiProcess_share_value() print(n.futures) 需求现在我们设定一个需求, 来看看两个语言下的future解决同一个问题是怎么处理的。需求是从各大网站爬取一些免费的代理IP,因为爬取的时候。各个任务在跑的时候是不相关的，所以我们可以用异步来执行这些，现在我们决定爬取的网站如下： http://www.xicidaili.com/nn/http://www.66ip.cn/http://www.nianshao.me/ 编码实现javajava在实现任务的时候，采用的spring boot, 但还是代码量太多，于是贴出只核心部分：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import com.legotime.constant.Constants;import com.legotime.domain.ProxyIP;import com.legotime.util.ProxyUtil;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.context.annotation.Configuration;import org.springframework.scheduling.annotation.Async;import org.springframework.scheduling.annotation.AsyncResult;import java.util.*;import java.util.concurrent.*;/*** 从网络爬取代理IP*/@Configurationpublic class FreeProxy &#123; private static final Logger log = LoggerFactory.getLogger(FreeProxy.class); public HashSet&lt;ProxyIP&gt; ipSet = new HashSet&lt;ProxyIP&gt;(); public FreeProxy() &#123; &#125; /** * 爬取 http://www.xicidaili.com/nn/ 的免费代理 * @return */ @Async private Future&lt;String&gt; getXicidaili()&#123; long start = System.currentTimeMillis(); log.info("开始爬取：http://www.xicidaili.com/nn/ 代理"); HashSet&lt;ProxyIP&gt; freeProxyIp = ProxyUtil.getFreeProxyIp(Constants.XICI_URL, Constants.XICI_PAGE, Constants.XICI_PAGE_SUF, Constants.XICI_1Feature, Constants.XICI_2Feature, Constants.XICI_IPIndex, Constants.XICI_PostIndex); ipSet.addAll(freeProxyIp); log.info("爬取：http://www.xicidaili.com/nn/ 代理 结束"); long end = System.currentTimeMillis(); return new AsyncResult&lt;&gt;("http://www.xicidaili.com/ 爬取结束，耗时:"+(end-start)+"毫秒"); &#125; /** * 爬取 http://www.66ip.cn/ 的免费代理 * @return */ @Async private Future&lt;String&gt; getIp66()&#123; long start = System.currentTimeMillis(); log.info("开始爬取：http://www.66ip.cn/ 代理"); HashSet&lt;ProxyIP&gt; freeProxyIp = ProxyUtil.getFreeProxyIp(Constants.IP66_URL, Constants.IP66_PAGE, Constants.IP66_PAGE_SUF, Constants.IP66_1Feature, Constants.IP66_2Feature, Constants.IP66_IPIndex, Constants.IP66_PostIndex); ipSet.addAll(freeProxyIp); log.info("爬取：http://www.66ip.cn/ 代理 结束"); long end = System.currentTimeMillis(); return new AsyncResult&lt;&gt;("http://www.66ip.cn/ 爬取结束，耗时:"+(end-start)+"毫秒"); &#125; /** * 爬取 http://www.nianshao.me 的免费代理 * @return */ @Async public Future&lt;String&gt; getNiaoshao()&#123; long start = System.currentTimeMillis(); log.info("开始爬取：http://www.nianshao.me 代理"); HashSet&lt;ProxyIP&gt; freeProxyIp = ProxyUtil.getFreeProxyIp(Constants.NIANSHAO_URL, Constants.NIANSHAO_PAGE, Constants.NIANSHAO_PAGE_SUF, Constants.NIANSHAO_1Feature, Constants.NIANSHAO_2Feature, Constants.NIANSHAO_IPIndex, Constants.NIANSHAO_PostIndex); ipSet.addAll(freeProxyIp); log.info("爬取：http://www.nianshao.me 代理 结束"); long end = System.currentTimeMillis(); return new AsyncResult&lt;&gt;("http://www.nianshao.me 爬取结束，耗时:"+(end-start)+"毫秒"); &#125; public void Train()&#123; long start = System.currentTimeMillis(); log.info("爬取代理任务开始，开始时间："+start); Future&lt;String&gt; xicidaili = getXicidaili(); Future&lt;String&gt; ip66 = getIp66(); while (true)&#123; if (getXicidaili().isDone() &amp;&amp; getIp66().isDone() &amp;&amp; getNiaoshao().isDone()) &#123; break; &#125; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; long end = System.currentTimeMillis(); log.info("爬取代理任务结束，结束时间："+end); log.info("任务全部完成总耗时"+(end-start)+"毫秒"); &#125; public HashSet&lt;ProxyIP&gt; GetAllIpProxy()&#123; Train(); return ipSet; &#125;&#125; python123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566# -*- coding: utf-8 -*-from utils.configUtil import constantfrom utils.spiderUtil import getFreeProxyimport concurrent.futuresimport datetimefrom utils.logUtil import Logclass GetWebProxy(object): """收集互联网的零散IP""" def __init__(self): super(GetWebProxy, self).__init__() self.cons = constant() self.futures = set() def get_xicidaili(self): return getFreeProxy( self.cons.xicidaili_url, self.cons.xicidaili_page, self.cons.xicidaili_page_suf, self.cons.xicidaili_1Feature, self.cons.xicidaili_2Feature, self.cons.xicidaili_ip_index, self.cons.xicidaili_post_index ) def get_ip66(self): return getFreeProxy( self.cons.ip66_url, self.cons.ip66_page, self.cons.ip66_page_suf, self.cons.ip66_1Feature, self.cons.ip66_2Feature, self.cons.ip66_ip_index, self.cons.ip66_post_index ) def get_nianshao(self): return getFreeProxy( self.cons.ip66_url, self.cons.ip66_page, self.cons.ip66_page_suf, self.cons.ip66_1Feature, self.cons.ip66_2Feature, self.cons.ip66_ip_index, self.cons.ip66_post_index ) def contro_func(self, task_func): return task_func def train(self): start_time = datetime.datetime.now() with concurrent.futures.ThreadPoolExecutor(max_workers=8) as exeutor: tasks = [self.get_xicidaili(), self.get_ip66(), self.get_nianshao()] future_task = &#123;exeutor.submit(self.contro_func, task): task for task in tasks&#125; for future in concurrent.futures.as_completed(future_task): # ip_ports = future_task[future] self.futures.update(set(future.result())) end_time = datetime.datetime.now() print("耗时：", (end_time-start_time)) # range(1, 10):耗时：耗时： 0:00:00.297792 range(1, 100000) 耗时： 0:00:00.402069if __name__ == '__main__': g = GetWebProxy() g.train() print(g.futures)]]></content>
      <categories>
        <category>-日常开发</category>
      </categories>
      <tags>
        <tag>-并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BGD、SGD和MBGD的一些区别]]></title>
    <url>%2F2017%2F09%2F12%2FBGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[梯度下降(Batch gradient descent)–BGD123456789101112131415161718192021# 梯度下降(Batch gradient descent)--BGDdef batch_gradient_descent(x, y, learn_rate, epoches): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ theta = np.array([0.0, 0.0]) for i in range(epoches): loss = [0.0, 0.0] # 全部的值带入，计算 梯度 m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 随机梯度下降(Stochastic gradient descent)–SGD12345678910111213141516171819202122232425262728293031323334353637383940414243# 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段def stochastic_gradient_descent_false(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] return batch_gradient_descent(x, y, learn_rate, epoches)# 正确的随机梯度应该是这样def stochastic_gradient_descent_true(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ theta = np.array([0.0, 0.0]) for i in range(epoches): shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] # 随机之后的值，进行梯度计算 loss = [0.0, 0.0] m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 小批量梯度下降(Mini-batch gradient descent)–MBGD1234567891011121314151617181920212223242526272829def mini_batch_gradient_descent(x, y, learn_rate, epoches, mini_length): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :param mini_length: mini batch length :return: """ # 随机打乱----optional theta = np.array([0.0, 0.0]) # 随机打乱数据 ----optional shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:, 0] x = shufflle_data[:, 1:3] for i in range(epoches): # 0-min_length， mini_length+1 2mini_length, ....... 一小段，一小段距离用于一次优化迭代 loss = [0.0, 0.0] for j in range(0, len(y), mini_length): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / mini_length loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / mini_length # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 实验代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199# -*- coding: utf-8 -*-# @Date : 2017/9/8# @Author : ryanbing (legotime@qq.com)import numpy as npimport matplotlib.pyplot as pltimport datetimerng = np.random.RandomState(1)x = 10 * rng.rand(500)y = 3 * x + 2 + rng.randn(500)# plt.scatter(x, y)# plt.show()# 找出 y = wx + b 中的w 和 b, 正确的应该是 w = 3, b = 2# 我们在计算的时候其看成 y = WX 其中 W= [w, b], X = [x, 1].T# 梯度下降(Batch gradient descent)--BGDdef batch_gradient_descent(x, y, learn_rate, epoches): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() theta = np.array([0.0, 0.0]) for i in range(epoches): loss = [0.0, 0.0] # 全部的值带入，计算 梯度 m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, theta# 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段def stochastic_gradient_descent_false(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] end_time = datetime.datetime.now() return end_time - start_time, batch_gradient_descent(x, y, learn_rate, epoches)# 正确的随机梯度应该是这样def stochastic_gradient_descent_true(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() theta = np.array([0.0, 0.0]) for i in range(epoches): shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] # 随机之后的值，进行梯度计算 loss = [0.0, 0.0] m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, theta# 小批量梯度下降(Mini-batch gradient descent)--MBGDdef mini_batch_gradient_descent(x, y, learn_rate, epoches, mini_length): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :param mini_length: mini batch length :return: """ start_time = datetime.datetime.now() # 随机打乱----optional theta = np.array([0.0, 0.0]) # 随机打乱数据 ----optional shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:, 0] x = shufflle_data[:, 1:3] for i in range(epoches): # 0-min_length， mini_length+1 2mini_length, ....... 一小段，一小段距离用于一次优化迭代 loss = [0.0, 0.0] for j in range(0, len(y), mini_length): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / mini_length loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / mini_length # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, thetadef contro_func(func, **kwargs): """ :param func: 函数 :param kwargs: func 中需要的参数 :return: """ x = kwargs.get('x', None) y = kwargs.get('y', None) learn_rate = kwargs.get('learn_rate', None) epoches = kwargs.get('epoches', None) stochastic_rate = kwargs.get('stochastic_rate', None) mini_length = kwargs.get('mini_length', None) # change the value is args is not num if stochastic_rate is not None: return func(x, y, learn_rate, epoches, stochastic_rate) if mini_length is not None: return func(x, y, learn_rate, epoches, mini_length) return func(x, y, learn_rate, epoches)def show_trend(): # 画出收敛的的图像和收敛对应的时间 rng = np.random.RandomState(1) x = 10 * rng.rand(500) x = np.array([x, np.ones(500)]).T y = 3 * x + 2 + rng.randn(500) learn_rate = 0.01 stochastic_rate = 0.4 mini_length = 10 for j in [batch_gradient_descent, stochastic_gradient_descent_false, stochastic_gradient_descent_true, mini_batch_gradient_descent]: tmp = [] for epoches in [1, 10, 100, 1000, 10000, 100000]: tmp.append(contro_func(i, x=x, y=y, learn_rate=learn_rate, stochastic_rate=stochastic_rate, mini_length=mini_length, epoches=epoches))if __name__ == '__main__': # test(func=func, x=1, y=2, learn_rate=3, epoches=4, stochastic_rate=5) # print(batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100000)) # [ 1.14378512 0.17288215] # [ 3.18801281 0.50870366] # [ 3.18602557 0.806018 ] # [ 3.03276102 1.84267445] # [ 3.01449298 1.96623647] # [ 3.01449298 1.96623647] # print(stochastic_gradient_descent_false(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100,stochastic_rate=0.4)) # [ 1.11939055 0.16949282] # [ 3.19877639 0.50404936] # [ 3.20921332 0.78698163] # [ 3.04720128 1.82412805] # [ 3.01920995 1.89883629] # [ 2.98281143 2.15226071] # print(stochastic_gradient_descent_true(np.array([x, np.ones(50000)]).T, y, learn_rate=0.01, epoches=1000,stochastic_rate=1)) # print(mini_batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100, mini_length=10)) # [ 0.94630842 0.14845568] # [ 0.8811451 0.15444328] # [ 3.18337012 0.51049921] # [ 3.14833317 0.79174635] # [ 3.03507147 1.87931184]]]></content>
      <categories>
        <category>-机器学习</category>
      </categories>
      <tags>
        <tag>-参数优化</tag>
      </tags>
  </entry>
</search>
