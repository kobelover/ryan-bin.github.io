<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[BGD、SGD和MBGD的一些区别]]></title>
    <url>%2F2017%2F09%2F12%2FBGD%E3%80%81SGD%E5%92%8CMBGD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[梯度下降(Batch gradient descent)–BGD123456789101112131415161718192021# 梯度下降(Batch gradient descent)--BGDdef batch_gradient_descent(x, y, learn_rate, epoches): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ theta = np.array([0.0, 0.0]) for i in range(epoches): loss = [0.0, 0.0] # 全部的值带入，计算 梯度 m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 随机梯度下降(Stochastic gradient descent)–SGD12345678910111213141516171819202122232425262728293031323334353637383940414243# 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段def stochastic_gradient_descent_false(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] return batch_gradient_descent(x, y, learn_rate, epoches)# 正确的随机梯度应该是这样def stochastic_gradient_descent_true(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ theta = np.array([0.0, 0.0]) for i in range(epoches): shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] # 随机之后的值，进行梯度计算 loss = [0.0, 0.0] m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 小批量梯度下降(Mini-batch gradient descent)–MBGD1234567891011121314151617181920212223242526272829def mini_batch_gradient_descent(x, y, learn_rate, epoches, mini_length): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :param mini_length: mini batch length :return: """ # 随机打乱----optional theta = np.array([0.0, 0.0]) # 随机打乱数据 ----optional shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:, 0] x = shufflle_data[:, 1:3] for i in range(epoches): # 0-min_length， mini_length+1 2mini_length, ....... 一小段，一小段距离用于一次优化迭代 loss = [0.0, 0.0] for j in range(0, len(y), mini_length): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / mini_length loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / mini_length # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] return theta 实验代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199# -*- coding: utf-8 -*-# @Date : 2017/9/8# @Author : ryanbing (legotime@qq.com)import numpy as npimport matplotlib.pyplot as pltimport datetimerng = np.random.RandomState(1)x = 10 * rng.rand(500)y = 3 * x + 2 + rng.randn(500)# plt.scatter(x, y)# plt.show()# 找出 y = wx + b 中的w 和 b, 正确的应该是 w = 3, b = 2# 我们在计算的时候其看成 y = WX 其中 W= [w, b], X = [x, 1].T# 梯度下降(Batch gradient descent)--BGDdef batch_gradient_descent(x, y, learn_rate, epoches): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() theta = np.array([0.0, 0.0]) for i in range(epoches): loss = [0.0, 0.0] # 全部的值带入，计算 梯度 m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, theta# 这不是随机梯度，随机梯度是每迭代一次，数据就随机一次---但是这也是一种处理手段def stochastic_gradient_descent_false(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] end_time = datetime.datetime.now() return end_time - start_time, batch_gradient_descent(x, y, learn_rate, epoches)# 正确的随机梯度应该是这样def stochastic_gradient_descent_true(x, y, learn_rate, epoches, stochastic_rate): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :return: """ start_time = datetime.datetime.now() theta = np.array([0.0, 0.0]) for i in range(epoches): shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) stochastic_count = int(len(y) * stochastic_rate) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:stochastic_count, 0] x = shufflle_data[:stochastic_count, 1:3] # 随机之后的值，进行梯度计算 loss = [0.0, 0.0] m = len(y) for j in range(m): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / m loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / m # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, theta# 小批量梯度下降(Mini-batch gradient descent)--MBGDdef mini_batch_gradient_descent(x, y, learn_rate, epoches, mini_length): """ :param x: 输入的x :param y: 输入的y :param learn_rate: 学习率 :param epoches: 迭代次数 :param mini_length: mini batch length :return: """ start_time = datetime.datetime.now() # 随机打乱----optional theta = np.array([0.0, 0.0]) # 随机打乱数据 ----optional shufflle_data = np.column_stack((y, x)) np.random.shuffle(shufflle_data) # 然后随机取一些数据进行梯度优化， 比如取随机100条数据 y = shufflle_data[:, 0] x = shufflle_data[:, 1:3] for i in range(epoches): # 0-min_length， mini_length+1 2mini_length, ....... 一小段，一小段距离用于一次优化迭代 loss = [0.0, 0.0] for j in range(0, len(y), mini_length): loss[0] = loss[0] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) * x[j, 0] / mini_length loss[1] = loss[1] + (theta[0] * x[j, 0] + theta[1] * x[j, 1] - y[j]) / mini_length # 更新 theta theta[0] = theta[0] - learn_rate * loss[0] theta[1] = theta[1] - learn_rate * loss[1] end_time = datetime.datetime.now() return end_time - start_time, thetadef contro_func(func, **kwargs): """ :param func: 函数 :param kwargs: func 中需要的参数 :return: """ x = kwargs.get('x', None) y = kwargs.get('y', None) learn_rate = kwargs.get('learn_rate', None) epoches = kwargs.get('epoches', None) stochastic_rate = kwargs.get('stochastic_rate', None) mini_length = kwargs.get('mini_length', None) # change the value is args is not num if stochastic_rate is not None: return func(x, y, learn_rate, epoches, stochastic_rate) if mini_length is not None: return func(x, y, learn_rate, epoches, mini_length) return func(x, y, learn_rate, epoches)def show_trend(): # 画出收敛的的图像和收敛对应的时间 rng = np.random.RandomState(1) x = 10 * rng.rand(500) x = np.array([x, np.ones(500)]).T y = 3 * x + 2 + rng.randn(500) learn_rate = 0.01 stochastic_rate = 0.4 mini_length = 10 for j in [batch_gradient_descent, stochastic_gradient_descent_false, stochastic_gradient_descent_true, mini_batch_gradient_descent]: tmp = [] for epoches in [1, 10, 100, 1000, 10000, 100000]: tmp.append(contro_func(i, x=x, y=y, learn_rate=learn_rate, stochastic_rate=stochastic_rate, mini_length=mini_length, epoches=epoches))if __name__ == '__main__': # test(func=func, x=1, y=2, learn_rate=3, epoches=4, stochastic_rate=5) # print(batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100000)) # [ 1.14378512 0.17288215] # [ 3.18801281 0.50870366] # [ 3.18602557 0.806018 ] # [ 3.03276102 1.84267445] # [ 3.01449298 1.96623647] # [ 3.01449298 1.96623647] # print(stochastic_gradient_descent_false(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100,stochastic_rate=0.4)) # [ 1.11939055 0.16949282] # [ 3.19877639 0.50404936] # [ 3.20921332 0.78698163] # [ 3.04720128 1.82412805] # [ 3.01920995 1.89883629] # [ 2.98281143 2.15226071] # print(stochastic_gradient_descent_true(np.array([x, np.ones(50000)]).T, y, learn_rate=0.01, epoches=1000,stochastic_rate=1)) # print(mini_batch_gradient_descent(np.array([x, np.ones(500)]).T, y, learn_rate=0.01, epoches=100, mini_length=10)) # [ 0.94630842 0.14845568] # [ 0.8811451 0.15444328] # [ 3.18337012 0.51049921] # [ 3.14833317 0.79174635] # [ 3.03507147 1.87931184]]]></content>
      <categories>
        <category>-机器学习</category>
      </categories>
      <tags>
        <tag>-参数优化</tag>
      </tags>
  </entry>
</search>
